{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Evaluation, Comparison & Analysis\n",
    "\n",
    "Load all trained models, run comprehensive cross-lingual evaluation, generate visualizations, and analyze results.\n",
    "\n",
    "**Prerequisites:** Run notebooks 01, 02, and 03 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "MODEL_DIR = \"./models\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "FIG_DIR = os.path.join(RESULTS_DIR, \"figures\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    \"en\": load_from_disk(os.path.join(DATA_DIR, \"en\"))[\"test\"],\n",
    "    \"fr\": load_from_disk(os.path.join(DATA_DIR, \"fr\"))[\"test\"],\n",
    "    \"nl\": load_from_disk(os.path.join(DATA_DIR, \"nl\"))[\"test\"],\n",
    "}\n",
    "\n",
    "for lang, ds in test_data.items():\n",
    "    print(f\"{lang.upper()} test set: {len(ds)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define All Models to Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configs: (display_name, model_dir, languages_to_evaluate_on)\n",
    "model_configs = [\n",
    "    # Monolingual models — only evaluate on their own language\n",
    "    (\"BERT (EN)\", os.path.join(MODEL_DIR, \"bert-en\"), [\"en\"]),\n",
    "    (\"BERTje (NL)\", os.path.join(MODEL_DIR, \"bertje-nl\"), [\"nl\"]),\n",
    "    (\"CamemBERT (FR)\", os.path.join(MODEL_DIR, \"camembert-fr\"), [\"fr\"]),\n",
    "    # mBERT variants — evaluate on all languages\n",
    "    (\"mBERT (EN only)\", os.path.join(MODEL_DIR, \"mbert-en-only\"), [\"en\", \"fr\", \"nl\"]),\n",
    "    (\"mBERT (FR only)\", os.path.join(MODEL_DIR, \"mbert-fr-only\"), [\"en\", \"fr\", \"nl\"]),\n",
    "    (\"mBERT (NL only)\", os.path.join(MODEL_DIR, \"mbert-nl-only\"), [\"en\", \"fr\", \"nl\"]),\n",
    "    (\"mBERT (EN+FR+NL)\", os.path.join(MODEL_DIR, \"mbert-multilingual\"), [\"en\", \"fr\", \"nl\"]),\n",
    "]\n",
    "\n",
    "# Check which models exist\n",
    "available_models = []\n",
    "for name, path, langs in model_configs:\n",
    "    if os.path.exists(path):\n",
    "        available_models.append((name, path, langs))\n",
    "        print(f\"  Found: {name}\")\n",
    "    else:\n",
    "        print(f\"  MISSING: {name} ({path})\")\n",
    "\n",
    "print(f\"\\n{len(available_models)}/{len(model_configs)} models available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, texts, labels, batch_size=32):\n",
    "    \"\"\"Run inference and return predictions.\"\"\"\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=model_path,\n",
    "        tokenizer=model_path,\n",
    "        device=0 if DEVICE == \"cuda\" else -1,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    results = classifier(texts)\n",
    "    # Map LABEL_0 -> 0, LABEL_1 -> 1\n",
    "    preds = [int(r[\"label\"].split(\"_\")[-1]) for r in results]\n",
    "    return preds\n",
    "\n",
    "\n",
    "all_eval_results = []\n",
    "\n",
    "for model_name, model_path, eval_langs in tqdm(available_models, desc=\"Models\"):\n",
    "    for lang in eval_langs:\n",
    "        texts = test_data[lang][\"text\"]\n",
    "        labels = test_data[lang][\"label\"]\n",
    "\n",
    "        preds = evaluate_model(model_path, texts, labels)\n",
    "\n",
    "        all_eval_results.append({\n",
    "            \"model\": model_name,\n",
    "            \"eval_lang\": lang,\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds),\n",
    "            \"precision\": precision_score(labels, preds),\n",
    "            \"recall\": recall_score(labels, preds),\n",
    "            \"predictions\": preds,  # Keep for error analysis\n",
    "            \"labels\": labels,\n",
    "        })\n",
    "\n",
    "        print(f\"  {model_name} on {lang.upper()}: Acc={all_eval_results[-1]['accuracy']:.4f}, F1={all_eval_results[-1]['f1']:.4f}\")\n",
    "\n",
    "    # Free GPU memory between models\n",
    "    torch.cuda.empty_cache() if DEVICE == \"cuda\" else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Results Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build clean results table (without predictions/labels columns)\n",
    "df_results = pd.DataFrame([\n",
    "    {k: v for k, v in r.items() if k not in (\"predictions\", \"labels\")}\n",
    "    for r in all_eval_results\n",
    "])\n",
    "\n",
    "# Pivot for readability\n",
    "pivot_f1 = df_results.pivot(index=\"model\", columns=\"eval_lang\", values=\"f1\")\n",
    "pivot_f1.columns = [f\"{c.upper()} F1\" for c in pivot_f1.columns]\n",
    "\n",
    "pivot_acc = df_results.pivot(index=\"model\", columns=\"eval_lang\", values=\"accuracy\")\n",
    "pivot_acc.columns = [f\"{c.upper()} Acc\" for c in pivot_acc.columns]\n",
    "\n",
    "full_table = pd.concat([pivot_f1, pivot_acc], axis=1)\n",
    "\n",
    "print(\"Full Evaluation Matrix\")\n",
    "print(\"=\" * 80)\n",
    "print(full_table.to_string(float_format=\"{:.4f}\".format))\n",
    "\n",
    "# Save\n",
    "full_table.to_csv(os.path.join(RESULTS_DIR, \"full_evaluation_matrix.csv\"))\n",
    "df_results.to_csv(os.path.join(RESULTS_DIR, \"detailed_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: F1 Score Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of F1 scores (model × evaluation language)\n",
    "heatmap_df = df_results.pivot(index=\"model\", columns=\"eval_lang\", values=\"f1\")\n",
    "heatmap_df.columns = [\"English\", \"French\", \"Dutch\"]\n",
    "\n",
    "# Order: monolingual first, then mBERT variants\n",
    "order = [\n",
    "    \"BERT (EN)\", \"BERTje (NL)\", \"CamemBERT (FR)\",\n",
    "    \"mBERT (EN only)\", \"mBERT (FR only)\", \"mBERT (NL only)\", \"mBERT (EN+FR+NL)\",\n",
    "]\n",
    "heatmap_df = heatmap_df.reindex([m for m in order if m in heatmap_df.index])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    vmin=0.5,\n",
    "    vmax=1.0,\n",
    "    linewidths=0.5,\n",
    "    ax=ax,\n",
    "    mask=heatmap_df.isna(),\n",
    ")\n",
    "ax.set_title(\"Sentiment Classification F1 Scores\\n(Monolingual vs. Multilingual Models)\", fontsize=13)\n",
    "ax.set_ylabel(\"Model (Training Config)\")\n",
    "ax.set_xlabel(\"Evaluation Language\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"f1_heatmap_all_models.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Monolingual vs. mBERT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: language-specific model vs. mBERT (all langs) per language\n",
    "comparison = {\n",
    "    \"English\": {\n",
    "        \"Monolingual (BERT)\": df_results[(df_results[\"model\"] == \"BERT (EN)\") & (df_results[\"eval_lang\"] == \"en\")][\"f1\"].values,\n",
    "        \"mBERT (EN only)\": df_results[(df_results[\"model\"] == \"mBERT (EN only)\") & (df_results[\"eval_lang\"] == \"en\")][\"f1\"].values,\n",
    "        \"mBERT (all langs)\": df_results[(df_results[\"model\"] == \"mBERT (EN+FR+NL)\") & (df_results[\"eval_lang\"] == \"en\")][\"f1\"].values,\n",
    "    },\n",
    "    \"French\": {\n",
    "        \"Monolingual (CamemBERT)\": df_results[(df_results[\"model\"] == \"CamemBERT (FR)\") & (df_results[\"eval_lang\"] == \"fr\")][\"f1\"].values,\n",
    "        \"mBERT (FR only)\": df_results[(df_results[\"model\"] == \"mBERT (FR only)\") & (df_results[\"eval_lang\"] == \"fr\")][\"f1\"].values,\n",
    "        \"mBERT (all langs)\": df_results[(df_results[\"model\"] == \"mBERT (EN+FR+NL)\") & (df_results[\"eval_lang\"] == \"fr\")][\"f1\"].values,\n",
    "    },\n",
    "    \"Dutch\": {\n",
    "        \"Monolingual (BERTje)\": df_results[(df_results[\"model\"] == \"BERTje (NL)\") & (df_results[\"eval_lang\"] == \"nl\")][\"f1\"].values,\n",
    "        \"mBERT (NL only)\": df_results[(df_results[\"model\"] == \"mBERT (NL only)\") & (df_results[\"eval_lang\"] == \"nl\")][\"f1\"].values,\n",
    "        \"mBERT (all langs)\": df_results[(df_results[\"model\"] == \"mBERT (EN+FR+NL)\") & (df_results[\"eval_lang\"] == \"nl\")][\"f1\"].values,\n",
    "    },\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5), sharey=True)\n",
    "colors = [\"#2196F3\", \"#FF9800\", \"#4CAF50\"]\n",
    "\n",
    "for ax, (lang, scores) in zip(axes, comparison.items()):\n",
    "    names = list(scores.keys())\n",
    "    vals = [v[0] if len(v) > 0 else 0 for v in scores.values()]\n",
    "    bars = ax.bar(range(len(names)), vals, color=colors)\n",
    "    ax.set_xticks(range(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=30, ha=\"right\", fontsize=9)\n",
    "    ax.set_title(lang, fontsize=12)\n",
    "    ax.set_ylim(0.7, 1.0)\n",
    "    ax.set_ylabel(\"F1 Score\" if ax == axes[0] else \"\")\n",
    "    for bar, val in zip(bars, vals):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
    "                f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "fig.suptitle(\"Monolingual vs. Multilingual Model Comparison (F1)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"monolingual_vs_multilingual.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Lingual Transfer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze zero-shot transfer degradation\n",
    "print(\"Cross-Lingual Transfer Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For each mBERT single-language model, compare in-language vs. cross-language performance\n",
    "transfer_configs = [\n",
    "    (\"mBERT (EN only)\", \"en\"),\n",
    "    (\"mBERT (FR only)\", \"fr\"),\n",
    "    (\"mBERT (NL only)\", \"nl\"),\n",
    "]\n",
    "\n",
    "for model_name, train_lang in transfer_configs:\n",
    "    model_rows = df_results[df_results[\"model\"] == model_name]\n",
    "    if model_rows.empty:\n",
    "        continue\n",
    "\n",
    "    in_lang_f1 = model_rows[model_rows[\"eval_lang\"] == train_lang][\"f1\"].values\n",
    "    if len(in_lang_f1) == 0:\n",
    "        continue\n",
    "    in_lang_f1 = in_lang_f1[0]\n",
    "\n",
    "    print(f\"\\n{model_name} (trained on {train_lang.upper()})\")\n",
    "    print(f\"  In-language F1: {in_lang_f1:.4f}\")\n",
    "\n",
    "    for _, row in model_rows.iterrows():\n",
    "        if row[\"eval_lang\"] != train_lang:\n",
    "            drop = in_lang_f1 - row[\"f1\"]\n",
    "            print(f\"  → {row['eval_lang'].upper()} F1: {row['f1']:.4f} (drop: {drop:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for mBERT (all langs) on each language\n",
    "mbert_all_results = [r for r in all_eval_results if r[\"model\"] == \"mBERT (EN+FR+NL)\"]\n",
    "\n",
    "if mbert_all_results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    lang_names = {\"en\": \"English\", \"fr\": \"French\", \"nl\": \"Dutch\"}\n",
    "\n",
    "    for ax, result in zip(axes, mbert_all_results):\n",
    "        cm = confusion_matrix(result[\"labels\"], result[\"predictions\"])\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Negative\", \"Positive\"],\n",
    "            yticklabels=[\"Negative\", \"Positive\"],\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(f\"mBERT (all langs) — {lang_names[result['eval_lang']]}\")\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_DIR, \"confusion_matrices_mbert_all.png\"), dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"mBERT (EN+FR+NL) results not found. Run notebook 03 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample misclassified examples from mBERT (all langs)\n",
    "if mbert_all_results:\n",
    "    for result in mbert_all_results:\n",
    "        lang = result[\"eval_lang\"]\n",
    "        preds = result[\"predictions\"]\n",
    "        labels = result[\"labels\"]\n",
    "        texts = test_data[lang][\"text\"]\n",
    "\n",
    "        # Find misclassified examples\n",
    "        misclassified_idx = [i for i, (p, l) in enumerate(zip(preds, labels)) if p != l]\n",
    "        n_errors = len(misclassified_idx)\n",
    "        total = len(labels)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{lang.upper()}: {n_errors}/{total} misclassified ({n_errors/total*100:.1f}%)\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Show 3 random misclassified examples\n",
    "        np.random.seed(42)\n",
    "        sample_idx = np.random.choice(misclassified_idx, min(3, len(misclassified_idx)), replace=False)\n",
    "\n",
    "        label_names = [\"Negative\", \"Positive\"]\n",
    "        for idx in sample_idx:\n",
    "            print(f\"\\n  Text: {texts[idx][:200]}...\")\n",
    "            print(f\"  True: {label_names[labels[idx]]}, Predicted: {label_names[preds[idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Q1: Zero-shot transfer quality\n",
    "print(\"Q1: How well does mBERT transfer across languages (zero-shot)?\")\n",
    "en_only = df_results[df_results[\"model\"] == \"mBERT (EN only)\"]\n",
    "if not en_only.empty:\n",
    "    for _, row in en_only.iterrows():\n",
    "        marker = \"(in-language)\" if row[\"eval_lang\"] == \"en\" else \"(zero-shot)\"\n",
    "        print(f\"  {row['eval_lang'].upper()} F1: {row['f1']:.4f} {marker}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Q2: Multilingual training benefit\n",
    "print(\"Q2: Does multilingual training improve over single-language?\")\n",
    "for lang in [\"en\", \"fr\", \"nl\"]:\n",
    "    single = df_results[(df_results[\"model\"] == f\"mBERT ({lang.upper()} only)\") & (df_results[\"eval_lang\"] == lang)][\"f1\"].values\n",
    "    multi = df_results[(df_results[\"model\"] == \"mBERT (EN+FR+NL)\") & (df_results[\"eval_lang\"] == lang)][\"f1\"].values\n",
    "    if len(single) > 0 and len(multi) > 0:\n",
    "        diff = multi[0] - single[0]\n",
    "        print(f\"  {lang.upper()}: single={single[0]:.4f} → multi={multi[0]:.4f} (diff: {diff:+.4f})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Q3: Monolingual specialists vs. mBERT\n",
    "print(\"Q3: Monolingual specialists vs. mBERT (in-language)?\")\n",
    "mono_models = {\n",
    "    \"en\": \"BERT (EN)\",\n",
    "    \"fr\": \"CamemBERT (FR)\",\n",
    "    \"nl\": \"BERTje (NL)\",\n",
    "}\n",
    "for lang, mono_name in mono_models.items():\n",
    "    mono_f1 = df_results[(df_results[\"model\"] == mono_name) & (df_results[\"eval_lang\"] == lang)][\"f1\"].values\n",
    "    multi_f1 = df_results[(df_results[\"model\"] == \"mBERT (EN+FR+NL)\") & (df_results[\"eval_lang\"] == lang)][\"f1\"].values\n",
    "    if len(mono_f1) > 0 and len(multi_f1) > 0:\n",
    "        diff = multi_f1[0] - mono_f1[0]\n",
    "        winner = \"mBERT\" if diff > 0 else mono_name\n",
    "        print(f\"  {lang.upper()}: {mono_name}={mono_f1[0]:.4f} vs mBERT={multi_f1[0]:.4f} → Winner: {winner}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All results saved to ./results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
